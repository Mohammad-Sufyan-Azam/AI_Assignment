{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7011f3d0",
   "metadata": {},
   "source": [
    "## Importing All Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a756fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import warnings\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa5ba7",
   "metadata": {},
   "source": [
    "## Downloading stopwords, punctuations, and wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe564a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\MOHD\n",
      "[nltk_data]     SUFYAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\MOHD\n",
      "[nltk_data]     SUFYAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\MOHD\n",
      "[nltk_data]     SUFYAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\MOHD\n",
      "[nltk_data]     SUFYAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf7b2af",
   "metadata": {},
   "source": [
    "## Reading Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9f8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules-\n",
    "# Only one interest should be mentioned ideally.\n",
    "# No spelling mistakes. Writes exact career and interest as defined in the list.\n",
    "# Write 'courses done' or 'done courses' preceeding the courses done by you.\n",
    "\n",
    "input_file = open(\"D:\\My Folder\\Academics\\AI\\AI_Assignment\\AI Assignment 5\\input_file.txt\", 'r')\n",
    "text = input_file.read()\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7fff65",
   "metadata": {},
   "source": [
    "## Preprocessing Input File Using NLP Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790c794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'name', 'sufyan', '3rd', 'year', 'undergrad', 'iiitd', 'goal', 'become', 'network', 'security', 'engineer', 'done', 'courses', 'like', 'sml', 'os', 'tais', 'mtl', 'loved', 'courses', 'provided', 'lot', 'valuable', 'information', 'things', 'work', 'grass', 'root', 'level', 'ai', 'system', 'internet', 'also', 'cgpa', '8', 'suggest', 'electives', 'help', 'achieving', 'goal']\n"
     ]
    }
   ],
   "source": [
    "input_words = []\n",
    "\n",
    "# stop words from English\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Declaring the wordNet Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = text.lower()\n",
    "# Removing the punctuations\n",
    "for punctuation in string.punctuation:\n",
    "  text = text.replace(punctuation, ' ')\n",
    "\n",
    "# lemmatising the words and updating the variable text\n",
    "text = wordnet_lemmatizer.lemmatize(text)\n",
    "\n",
    "# tokenising text\n",
    "tokenised_text = word_tokenize(text)\n",
    "\n",
    "# removing the stop words here\n",
    "for word in tokenised_text:\n",
    "    if word not in stopWords:\n",
    "      input_words.append(word)\n",
    "\n",
    "print(input_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4228212",
   "metadata": {},
   "source": [
    "## Defining Courses and Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b2ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_choices = ['Network and Security Engineer', 'Data Engineer', 'Electronics Engineer', 'Bioinformatics Engineer', \n",
    "                    'Robotics Engineer', 'AI Engineer', 'ML Engineer']\n",
    "\n",
    "data_engineer_courses = ['Database Management Systems (DBMS)', 'Database System Implementation (DBSI)', 'Big Data Analytics (BDA)', 'Data Science (DSC)']\n",
    "ai_engineer_courses = ['Artificial Intelligence (AI)', 'Meta-Learning (MTL)', 'Trustworthy AI Systems (TAIS)']\n",
    "ml_engineer_courses = ['Statistical Machine Learning (SML)', 'Advanced Machine Learning (AML)', 'Machine Learning (ML)', \n",
    "                       'Natural Language Processing (NLP)']\n",
    "robotics_engineer_courses = ['Robotics (IRob)', 'Social Robotics (SR)', 'Non Linear and Adaptive Control of Robotic Systems (NLR)']\n",
    "electronics_engineer_courses = ['Integrated Electronics (IE)', 'Circuit theory and devices (CTD)', 'Fields and Waves (F&W)', \n",
    "                                'Embedded Logic Design (ELD)', 'Digital Signal Processing (DSP)']\n",
    "bioinformatics_engineer_courses = ['Practical Bioinformatics (PB)', 'Algorithms in BioInformatics (ABIN)', \n",
    "'Algorithms in Computational Biology (ACB)', 'Computing For Medicine (CM)', 'Computer Aided Drug Design (CADD)']\n",
    "network_engineer_courses = ['Computer Networks (CN)', 'Network Security (NSC)', 'Operating Systems (OS)', 'Mining Large Networks (MLN)', 'Network Anonymity and Privacy (NAP)']\n",
    "ai_engineer_courses += ml_engineer_courses\n",
    "\n",
    "list_of_courses = network_engineer_courses + data_engineer_courses + ai_engineer_courses + robotics_engineer_courses + electronics_engineer_courses + bioinformatics_engineer_courses \n",
    "# print(len(list_of_courses), list_of_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a03dc3",
   "metadata": {},
   "source": [
    "## Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfcea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word, listt):\n",
    "    try:\n",
    "        index = listt.index(word)\n",
    "        return index\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def find_career_interest(index, listt):\n",
    "    try:\n",
    "        i = index - 1\n",
    "        if listt[i] == 'security' and listt[i-1] == 'network':\n",
    "            return interest_choices[0]\n",
    "        elif listt[i] == 'data':\n",
    "            return interest_choices[1]\n",
    "        elif listt[i] == 'electronics':\n",
    "            return interest_choices[2]\n",
    "        elif listt[i] == 'bioinformatics':\n",
    "            return interest_choices[3]\n",
    "        elif listt[i] == 'robotics':\n",
    "            return interest_choices[4]\n",
    "        elif listt[i] == 'ai':\n",
    "            return interest_choices[5]\n",
    "        elif listt[i] == 'ml':\n",
    "            return interest_choices[6]\n",
    "    except:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "def find_courses_done(index, listt):\n",
    "    try:\n",
    "        if listt[index-1] != 'done' and listt[index+1] != 'done':\n",
    "            return []\n",
    "        \n",
    "        i = index\n",
    "        if listt[i-1] == 'done':\n",
    "            i = i+1\n",
    "        elif listt[i+1] == 'done':\n",
    "            i = i+2\n",
    "        \n",
    "        courses_done = []\n",
    "        stopping_words = ['become', 'becoming', 'career', 'interest', 'interested', 'cgpa', 'loved', 'goal']\n",
    "        while (i < len(listt)):\n",
    "            # stopping condition for courses done\n",
    "            for stop in stopping_words:\n",
    "                if listt[i] == stop:\n",
    "                    return courses_done\n",
    "            \n",
    "            courses_done.append(listt[i])\n",
    "            i += 1\n",
    "        return courses_done\n",
    "    except:\n",
    "        raise Exception\n",
    "\n",
    "\n",
    "def get_course_name(name):\n",
    "    for course_name in list_of_courses:\n",
    "        if name in course_name.lower():\n",
    "            return course_name\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3319db",
   "metadata": {},
   "source": [
    "## Extracting Career Interest and Courses Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234a27cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network and Security Engineer\n"
     ]
    }
   ],
   "source": [
    "interest = ''\n",
    "if 'engineer' in input_words:\n",
    "    index = input_words.index('engineer')\n",
    "    interest = find_career_interest(index, input_words)\n",
    "print(interest)\n",
    "\n",
    "courses_done = []\n",
    "if 'courses' in input_words:\n",
    "    index = input_words.index('courses')\n",
    "    courses_done = find_courses_done(index, input_words)\n",
    "# print(courses_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95d8a4",
   "metadata": {},
   "source": [
    "## Asserting Facts In a Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e0ca25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_file = open(\"input_facts.txt\", 'w')\n",
    "\n",
    "if len(interest) > 0:\n",
    "    assert_fact_interest = \"interest('\"+interest+\"').\\n\"\n",
    "    facts_file.write(assert_fact_interest)\n",
    "else:\n",
    "    assert_fact_interest = \"interest('None').\\n\"\n",
    "    facts_file.write(assert_fact_interest)\n",
    "\n",
    "\n",
    "if len(courses_done) > 0:\n",
    "    for name in courses_done:\n",
    "        if get_course_name(name) == 'None':\n",
    "            continue\n",
    "        assert_course_done = \"course_taken('\"+get_course_name(name)+\"').\\n\"\n",
    "        facts_file.write(assert_course_done)\n",
    "else:\n",
    "    assert_course_done = \"course_taken('None').\\n\"\n",
    "    facts_file.write(assert_course_done)\n",
    "\n",
    "\n",
    "for i in range(len(interest_choices)):\n",
    "    interest_choice_courses = \"interest_pre_requisite_courses('\"+interest_choices[i]+\"', [\"\n",
    "    if i == 0:\n",
    "        for course in network_engineer_courses:\n",
    "            interest_choice_courses += \"'\"+course+\"', \"\n",
    "    if i == 1:\n",
    "        for course in data_engineer_courses:\n",
    "            interest_choice_courses += \"'\"+course+\"', \"\n",
    "    if i == 2:\n",
    "        for course in electronics_engineer_courses:\n",
    "            interest_choice_courses += \"'\"+course+\"', \"\n",
    "    if i == 3:\n",
    "        for course in bioinformatics_engineer_courses:\n",
    "            interest_choice_courses += \"'\"+course+\"', \"\n",
    "    if i == 4:\n",
    "        for course in robotics_engineer_courses:\n",
    "            interest_choice_courses += \"'\"+course+\"', \"\n",
    "    if i == 5:\n",
    "        for course in ai_engineer_courses:\n",
    "            interest_choice_courses += \"'\"+course+\"', \"\n",
    "    if i == 6:\n",
    "        for course in ml_engineer_courses:\n",
    "            interest_choice_courses += \"'\"+course+\"', \"\n",
    "    interest_choice_courses = interest_choice_courses[:-2]\n",
    "    interest_choice_courses += \"]).\\n\"\n",
    "    \n",
    "    facts_file.write(interest_choice_courses)\n",
    "    \n",
    "\n",
    "facts_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb3dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyswip import Prolog\n",
    "\n",
    "# swipl = Prolog()\n",
    "# swipl.consult(\"main.pl\")\n",
    "# # swipl.query(\"get_facts.\")\n",
    "# electives_suggested = list(swipl.query(\"start\"))\n",
    "# print(electives_suggested)\n",
    "# # for i in range(len(electives_suggested)):\n",
    "# #     var = electives_suggested[i].decode('utf-8')\n",
    "# #     output(var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
